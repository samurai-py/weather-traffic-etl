
# Boas-vindas

Se vocÃª chegou atÃ© aqui, Ã© porque se interessou em saber mais sobre o projeto. Este arquivo tem duas partes: Os meus passos e dicas para vocÃª conseguir usÃ¡-lo, e as instruÃ§Ãµes padrÃµes da Astronomer.

### Projeto

Aqui, nÃ³s temos uma aplicaÃ§Ã£o que faz todo o processo de ETL acerca do clima das cidades do Vale do ParaÃ­ba, SÃ£o Paulo e Recife. AlÃ©m disso, trÃ¡s informaÃ§Ãµes sobre distÃ¢ncias e melhores rotas para viajar entre municÃ­pios. Os *jobs* rodam a cada uma hora, e eram mais de 1500 rotas calculadas, entÃ£o, por fins de construÃ§Ã£o, apenas informaÃ§Ãµes de trÃªs caminhos estÃ£o disponÃ­vels: `Recife` -> `SÃ£o Paulo`, `Recife` -> `SÃ£o JosÃ© dos Campos` e `SÃ£o Paulo` -> `SÃ£o JosÃ© dos Campos`.

Esses processo alimenta um banco de dados que Ã© consumido na aplicaÃ§Ã£o da **Zebrinha Azul**, que vocÃª pode conferir [aqui](https://zebrinha-azul-dash-app.onrender.com/).

O **Airflow** Ã© bem conhecido para orquestraÃ§Ã£o de pipelines de dados. Ele tem como base as chamas DAGs, que sÃ£o tarefas que rodam automaticamente. Aqui, alÃ©m do Airflow, utilizamos o `Astro SDK` para construir e gerenciar nosso ambiente. O Astro facilita principalmente nas tarefas de trocar fontes e destinos de dados, alÃ©m de acelerar a interaÃ§Ã£o entre *tasks*.

Temos duas DAGs principais:

- `database_pipeline.py`: Gerencia o funcionamento e garante que o banco de dados esteja sempre disponÃ­vel.
- `etl_pipeline.py`: Que condensa todas as operaÃ§Ãµes de extraÃ§Ã£o, transformaÃ§Ã£o, validaÃ§Ã£o e carregamento dos dados para o banco. Ã‰ o core do projeto.

Foram feitos pequenos testes para garantir a qualidade dos dados utilizando `great-expectations`. Eles rodam em todos os arquivos gerados automaticamente, assim que sÃ£o coletados das **APIs** e depois do processo de transformaÃ§Ã£o, antes das informaÃ§Ãµes serem carregadas no banco.

**Estrutura**

```
â”œâ”€â”€â”€.astro
â”œâ”€â”€â”€dags
â”‚   â””â”€â”€â”€src
â”‚       â”œâ”€â”€â”€data
â”‚       â”œâ”€â”€â”€etl
â”‚       â”‚   â”œâ”€â”€â”€operators
â”‚       â”‚   â”‚   â”œâ”€â”€â”€extractors
â”‚       â”‚   â”‚   â”œâ”€â”€â”€loaders
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€â”€database
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€â”€mapper
â”‚       â”‚   â”‚   â”œâ”€â”€â”€transformers
â”‚       â”‚   â”œâ”€â”€â”€sql
â”‚       â””â”€â”€â”€validators
â”‚           â””â”€â”€â”€validation_assets
â”‚               â””â”€â”€â”€cols
â”œâ”€â”€â”€include
|   â””...
â”œâ”€â”€â”€plugins
â””â”€â”€â”€tests
    â””â”€â”€â”€dags
```

A disponibilizaÃ§Ã£o de pastas do repositÃ³rio foi feita para ser bem modular e tornar independentes as funÃ§Ãµes de etl e validaÃ§Ã£o. Ã‰ complicado fazer dags rodarem scripts externos de python, entÃ£o tive que alocar tudo dentro da pasta.

As pastas `include` e `tests` sÃ£o dependÃªncias do prÃ³prio **Astro SDK**. TambÃ©m hÃ¡ um ambiente `great-expectations` no projeto, mas optei por construir os validadores do `ge` na pasta `validators`.

### Futuras melhorias

Pela falta de tempo em concluir o projeto, dÃ¡ para perceber que algumas coisas nÃ£o estÃ£o totalmente otimizadas. Apesar dos pipelines estarem consistentes, muitos parÃ¢metros nas funÃ§Ãµes (Principalmente de diretÃ³rios), podem ser otimizados.

No final do desenvolvimento, percebi que havia uma inconsistÃªncia nas saÃ­das de dados numÃ©ricos em uma das APIs. Isso foi tratado e passado para a etapa de **transformaÃ§Ã£o**, mas seria bom incluir esse caso nas validaÃ§Ãµes do *great-expectations.*

Criar uma DAG que avisa por e-mail quando alguma das validaÃ§Ãµes der errado.

CÃ³digo sempre pode ser refatorado. A ideia Ã© deixÃ¡-lo cada vez mais limpo.

HOSPEDAR NA ASTRONOMER NÃ‰ :D

***EDIT***: *JÃ¡ estÃ¡ hospedado. Positivo e operante 24/7 :p*

## RecomendaÃ§Ãµes para a InstalaÃ§Ã£o

- Eu utilizei um banco de dados Redshift, WeatherApi e a Api de Mapas da Google. VocÃª precisarÃ¡ configurar variÃ¡veis de ambiente para usÃ¡-los. Os campos necessÃ¡rios sÃ£o:

    - ``PYTHONPATH``='usr/local/airflow' (Recomendo deixar desse exato jeito porque Ã© bem complicado mexer com dependÃªncias no ambiente Airflow)
 
    **Credenciais do Redshift que vocÃª pega na AWS**
    - ``REDSHIFT_HOST`` 
    - ``REDSHIFT_PORT``
    - ``REDSHIFT_DBNAME``
    - ``REDSHIFT_USER``
    - ``REDSHIFT_PASSWORD``

    **Outros**
    - ``REDSHIFT_CONN_ID`` - Essa aqui Ã© o nome da conexÃ£o que vocÃª vai criar no Airflow UI. A aplicaÃ§Ã£o se conecta com o redshift por ela para rodar a DAG de criaÃ§Ã£o de tabelas

    - ``WEATHER_API_KEY`` - Chave da [WeatherAPI](https://www.weatherapi.com/), utilizada nesse projeto.
    - ``DIRECTIONS_API_KEY`` - Chave da [API Directions do Google](https://developers.google.com/maps/documentation/directions/overview)
- VocÃª tambÃ©m tem que ter o Python na sua mÃ¡quina para iniciar o ambiente com `astro dev init`, alÃ©m do **Docker Desktop**
- Todos os arquivos `.csv` sÃ£o gerados pelas dags, com exceÃ§Ã£o do `municipios.csv`, que eu adaptei manualmente porque os disponÃ­veis na internet continuam erros.

Tutorial da Astro aÃ­ embaixo. Flw ðŸš€

========

Overview
========

Welcome to Astronomer! This project was generated after you ran 'astro dev init' using the Astronomer CLI. This readme describes the contents of the project, as well as how to run Apache Airflow on your local machine.

Project Contents
================

Your Astro project contains the following files and folders:

- dags: This folder contains the Python files for your Airflow DAGs. By default, this directory includes two example DAGs:
    - `example_dag_basic`: This DAG shows a simple ETL data pipeline example with three TaskFlow API tasks that run daily.
    - `example_dag_advanced`: This advanced DAG showcases a variety of Airflow features like branching, Jinja templates, task groups and several Airflow operators.
- Dockerfile: This file contains a versioned Astro Runtime Docker image that provides a differentiated Airflow experience. If you want to execute other commands or overrides at runtime, specify them here.
- include: This folder contains any additional files that you want to include as part of your project. It is empty by default.
- packages.txt: Install OS-level packages needed for your project by adding them to this file. It is empty by default.
- requirements.txt: Install Python packages needed for your project by adding them to this file. It is empty by default.
- plugins: Add custom or community plugins for your project to this file. It is empty by default.
- airflow_settings.yaml: Use this local-only file to specify Airflow Connections, Variables, and Pools instead of entering them in the Airflow UI as you develop DAGs in this project.

Deploy Your Project Locally
===========================

1. Start Airflow on your local machine by running 'astro dev start'.

This command will spin up 4 Docker containers on your machine, each for a different Airflow component:

- Postgres: Airflow's Metadata Database
- Webserver: The Airflow component responsible for rendering the Airflow UI
- Scheduler: The Airflow component responsible for monitoring and triggering tasks
- Triggerer: The Airflow component responsible for triggering deferred tasks

2. Verify that all 4 Docker containers were created by running 'docker ps'.

Note: Running 'astro dev start' will start your project with the Airflow Webserver exposed at port 8080 and Postgres exposed at port 5432. If you already have either of those ports allocated, you can either [stop your existing Docker containers or change the port](https://docs.astronomer.io/astro/test-and-troubleshoot-locally#ports-are-not-available).

3. Access the Airflow UI for your local Airflow project. To do so, go to http://localhost:8080/ and log in with 'admin' for both your Username and Password.

You should also be able to access your Postgres Database at 'localhost:5432/postgres'.

Deploy Your Project to Astronomer
=================================

If you have an Astronomer account, pushing code to a Deployment on Astronomer is simple. For deploying instructions, refer to Astronomer documentation: https://docs.astronomer.io/cloud/deploy-code/

Contact
=======

The Astronomer CLI is maintained with love by the Astronomer team. To report a bug or suggest a change, reach out to our support.
